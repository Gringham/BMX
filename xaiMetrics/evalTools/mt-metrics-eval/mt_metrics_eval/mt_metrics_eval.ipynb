{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUPVve909iM8"
      },
      "source": [
        "# Demo colab for mt_metrics_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "Cr0TM9EY7wOH"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "from mt_metrics_eval import meta_info\n",
        "from mt_metrics_eval import data\n",
        "from mt_metrics_eval import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "GznnWylA8gwJ"
      },
      "outputs": [],
      "source": [
        "# @title Print all available evalsets, load selected wmt21 sets\n",
        "\n",
        "all_evs = {}  # name/lp -\u003e evs\n",
        "for testset in meta_info.DATA:\n",
        "  print(f'{testset}:', ' '.join(lp for lp in meta_info.DATA[testset]))\n",
        "  if testset in ('wmt21.news', 'wmt21.tedtalks'):\n",
        "    for lp in meta_info.DATA[testset]:\n",
        "      evs = data.EvalSet(testset, lp, True)\n",
        "      all_evs[f'{testset}/{lp}'] = evs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "jAuNRBA79Yai"
      },
      "outputs": [],
      "source": [
        "# @title Print info for all loaded evalsets\n",
        "\n",
        "print(f'{\"name\":\u003c20}  segs sys metrics gold  refs std')\n",
        "for name, evs in all_evs.items():\n",
        "  nsegs = len(evs.src)\n",
        "  nsys = len(evs.sys_names)\n",
        "  nmetrics = len(evs.metric_basenames)\n",
        "  gold = evs.StdHumanScoreName('sys')\n",
        "  nrefs = len(evs.ref_names)\n",
        "  std_ref = evs.std_ref\n",
        "\n",
        "  print(f'{name:\u003c20} {nsegs:5d} {nsys:3d} {nmetrics:7d} '\n",
        "        f'{gold:5} {nrefs:4d} {std_ref}') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJcvvmU9hTrZ"
      },
      "outputs": [],
      "source": [
        "# @title Correlations and significance matrix for wmt21.news en-de\n",
        "# System-level Pearson MQM correlations and significance matrix for with human\n",
        "# translations included in scoring, using primary metric submissions only.\n",
        "# Takes about 20s due to bootstrapping for significance tests.\n",
        "\n",
        "# Get map from metric-name -\u003e 'Correlation' objects containing sufficient stats.\n",
        "evs = all_evs['wmt21.news/en-de']\n",
        "level = 'sys'\n",
        "corrs = data.GetCorrelations(\n",
        "    evs=evs,\n",
        "    level=level,\n",
        "    main_refs={evs.std_ref},\n",
        "    close_refs={'refB'},\n",
        "    include_human=True,\n",
        "    include_outliers=False,\n",
        "    gold_name=evs.StdHumanScoreName(level),\n",
        "    primary_metrics=True)\n",
        "\n",
        "# Compute Pearson correlations and pairwise significance matrix.\n",
        "corr_map, sig_matrix = data.CompareMetrics(corrs, scipy.stats.pearsonr)\n",
        "\n",
        "print('System-level +HT Pearson correlations and ranks for wmt21.news en-de:') \n",
        "for m, (corr_val, rank) in corr_map.items():\n",
        "  print(f'{m:\u003c21} {corr_val: 0.3f} {rank}')\n",
        "print()\n",
        "print('Significant differences in Pearson correlation:')\n",
        "n = len(corr_map)\n",
        "for i in range(n):\n",
        "  better = ['\u003e' if sig_matrix[i, j] \u003c 0.05 else '=' for j in range(i + 1, n)]\n",
        "  better = ['.'] * (n - len(better)) + better\n",
        "  print(f'{ranked_metrics[i]:\u003c22} {\" \".join(better)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtuqYAmM5glQ"
      },
      "outputs": [],
      "source": [
        "# @title Rank-wise comparison of metrics across tasks\n",
        "\n",
        "# Selecting only domains \u0026 language pairs for which we have MQM scores.\n",
        "# Note: Using a low k value so tests finish faster. Set k higher for more stable\n",
        "# conclusions.\n",
        "\n",
        "tasks = []  # list of task names\n",
        "aggregate_ranks = {}  # metric -\u003e list of ranks across tasks\n",
        "\n",
        "for domain in 'wmt21.news', 'wmt21.tedtalks':\n",
        "  for lp in 'en-de', 'en-ru', 'zh-en':\n",
        "    evs_name = f'{domain}/{lp}'\n",
        "    evs = all_evs[evs_name]\n",
        "    close_refs = {'refB'} if evs_name == 'wmt21.news/en-de' else set()\n",
        "    for level in 'sys', 'seg':\n",
        "      for avg in 'none', 'sys':\n",
        "        if level == 'sys' and avg == 'sys': continue\n",
        "        for human in True, False:\n",
        "          if human == True and domain == 'wmt21.tedtalks': continue\n",
        "          for corr_fcn in scipy.stats.pearsonr, scipy.stats.kendalltau:\n",
        "            task_name = f'{evs_name}.{level}.avg_by_{avg}'\n",
        "            task_name += f'.{\"HT\" if human else \"noHT\"}'\n",
        "            task_name += f'.{corr_fcn.__name__}'\n",
        "            corrs = data.GetCorrelations(\n",
        "                evs=evs,\n",
        "                level=level,\n",
        "                main_refs={evs.std_ref},\n",
        "                close_refs=close_refs,\n",
        "                include_human=human,\n",
        "                include_outliers=False,\n",
        "                gold_name='mqm',\n",
        "                primary_metrics=True)\n",
        "            tasks.append(task_name)\n",
        "\n",
        "            corrs_and_ranks, _ = data.CompareMetrics(\n",
        "                corrs, corr_fcn, average_by=avg, k=5, pval=0.05)\n",
        "            for m in corrs_and_ranks:\n",
        "              name, metric_refs = evs.ParseMetricName(m)\n",
        "              if not metric_refs:\n",
        "                name = m  # Distinguish reference-free versions of metrics.\n",
        "              if name not in aggregate_ranks:\n",
        "                aggregate_ranks[name] = []\n",
        "              missing_entries = len(tasks) - 1 - len(aggregate_ranks[name])\n",
        "              aggregate_ranks[name] += [None] * missing_entries\n",
        "              aggregate_ranks[name].append(corrs_and_ranks[m][1])\n",
        "            print('.', end='')\n",
        "print()\n",
        "\n",
        "for m in aggregate_ranks:\n",
        "  aggregate_ranks[m] += [None] * (len(tasks) - len(aggregate_ranks[m]))\n",
        "\n",
        "def avg_no_nones(scores):\n",
        "  scores = [s for s in scores if s is not None]\n",
        "  return sum(scores) / len(scores)\n",
        "\n",
        "aggregate_ranks = dict(\n",
        "    sorted(aggregate_ranks.items(), key=lambda x: avg_no_nones(x[1])))\n",
        "\n",
        "print('task key:')\n",
        "for i, t in enumerate(tasks):\n",
        "  print(f'{i:\u003c2} {t}')\n",
        "print()\n",
        "\n",
        "for m, ranks in aggregate_ranks.items():\n",
        "  n = sum(r is not None for r in ranks)\n",
        "  avg = avg_no_nones(ranks)\n",
        "  ranks_str = ' '.join(['  ' if r is None else f'{r:2d}' for r in ranks])\n",
        "  print(f'{m:\u003c22} {avg:6.3f} ({n} tasks) {ranks_str}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "mt-metrics-eval.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1gXA-HQKMF6G4IdrUob8hPnbVm6_rsfeX",
          "timestamp": 1656987947120
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
