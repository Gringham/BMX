{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIJuxJU2ljId"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b-x-9Hw5yye"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import glob\n",
        "from mt_metrics_eval import meta_info\n",
        "from mt_metrics_eval import data\n",
        "from mt_metrics_eval import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_aqZcTTEI2e"
      },
      "outputs": [],
      "source": [
        "# @title Task-handling utils\n",
        "\n",
        "def task_to_dict(taskname):\n",
        "  \"\"\"Convert a task name into an attribute:value dict.\"\"\"\n",
        "  return dict(tuple(s.split('=')) for s in taskname.split())\n",
        "\n",
        "def get_val(taskname, attr):\n",
        "  \"\"\"Get the value for a given attribution from a task name.\"\"\"\n",
        "  return task_to_dict(taskname)[attr]\n",
        "\n",
        "def attr_vals(tasknames):\n",
        "  \"\"\"Return an attribute: {values} map for a collection of task names.\"\"\"\n",
        "  attr_val_dict = {}\n",
        "  for name in tasknames:\n",
        "    for k, v in task_to_dict(taskname):\n",
        "      if k not in attr_val_dict:\n",
        "        attr_val_dict[k] = set()\n",
        "      attr_val_dict[k].add(v)\n",
        "  return attr_val_dict\n",
        "\n",
        "def partition_by_attribute(tasknames, attr):\n",
        "  \"\"\"Partition a collection of task names by values of a given attribute.\"\"\"\n",
        "  partition = {}  # val -\u003e [task names where attr = val]\n",
        "  for name in tasknames:\n",
        "    attr_val = get_val(name, attr)\n",
        "    if attr_val not in partition:\n",
        "      partition[attr_val] = []\n",
        "    partition[attr_val].append(name)\n",
        "  return partition\n",
        "\n",
        "def sort_by_attrs(tasknames, attr_list):\n",
        "  \"\"\"Sort a collection of tasknames by attributes, in order.\"\"\"\n",
        "  if not attr_list:\n",
        "    return tasknames\n",
        "  ret = []\n",
        "  partition = partition_by_attribute(tasknames, attr_list[0])\n",
        "  for names in partition.values():\n",
        "    ret.extend(sort_by_attrs(names, attr_list[1:]))\n",
        "  return ret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qvs0KAX9f4Q"
      },
      "outputs": [],
      "source": [
        "# @title Main evaluation function\n",
        "\n",
        "def reformat(results):\n",
        "  \"\"\"Reformat CompareMetrics() results to match mtme's format.\"\"\"\n",
        "  metrics, sig_matrix = results\n",
        "  res = {}\n",
        "  for i, (m, (corr, rank)) in enumerate(metrics.items()):\n",
        "    sigs = ['1' if p \u003c 0.05 else '0' for p in sig_matrix[i]]\n",
        "    sigs = ['x'] * (i + 1) + sigs[i + 1:]\n",
        "    res[m] = (rank, corr, ' '.join(sigs))\n",
        "  return res\n",
        "\n",
        "def eval_metrics(eval_sets, langs, levels, primary_only, k, gold_name='std',\n",
        "                 include_domains=True, seg_level_no_avg=False,\n",
        "                 include_human_with_acc=False):\n",
        "  \"\"\"Evaluate all metrics for eval sets, across multiple task settings.\n",
        "\n",
        "  Args:\n",
        "    eval_sets: Map from lang-pair to eval_set objects.\n",
        "    langs: List of language pairs (eg 'en-de') for which to compute results.\n",
        "    levels: List of levels for which to compute results, allowed elements are\n",
        "      'sys' and 'seg'.\n",
        "    primary_only: Include only primary metrics.\n",
        "    k: Number of boostrap draws. If 0, no significance tests for metric-score\n",
        "      differences are run, and execution is much faster.\n",
        "    gold_name: Name of gold scores to use, standard scores if 'std'.\n",
        "    include_domains: Generate domain-specific results in addition to global\n",
        "      results.\n",
        "    seg_level_no_avg: If True, use only the average_by=None setting for segment-\n",
        "      level correlations\n",
        "    include_human_with_acc: If True, include human outputs in accuracy tasks.\n",
        "\n",
        "  Returns:\n",
        "    Map from task names to metric -\u003e (rank, corr, sig_string) stats.\n",
        "  \"\"\"\n",
        "  results = {}\n",
        "\n",
        "  # First task is global accuracy, iff more than one language is given.\n",
        "  if len(langs) \u003e 1:\n",
        "    evs_list = [eval_sets[lp] for lp in langs]\n",
        "    main_refs = [{evs.std_ref} for evs in evs_list]\n",
        "    close_refs = [set() for evs in evs_list]\n",
        "    if gold_name == 'std':\n",
        "      gold = evs_list[0].StdHumanScoreName('sys')\n",
        "    else:\n",
        "      gold = gold_name\n",
        "    humans = [True, False] if include_human_with_acc else [False]\n",
        "    for human in humans:\n",
        "      taskname = data.MakeTaskName(\n",
        "          'wmt22', langs, None, 'sys', human, 'none', 'accuracy', k, gold,\n",
        "          main_refs, close_refs, False, primary_only)\n",
        "      print(taskname)\n",
        "      res = data.CompareMetricsWithGlobalAccuracy(\n",
        "          evs_list, main_refs, close_refs, include_human=human,\n",
        "          include_outliers=False, gold_name=gold,\n",
        "          primary_metrics=primary_only,\n",
        "          domain=None, k=k, pval=0.05)\n",
        "      results[taskname] = reformat(res)\n",
        "  \n",
        "  # Remaining tasks are specific to language, domain, etc.\n",
        "  for lp in langs:\n",
        "    evs = eval_sets[lp]\n",
        "    main_refs = {evs.std_ref}\n",
        "    close_refs = set()\n",
        "    for domain in [None] + (list(evs.domain_names) if include_domains else []):\n",
        "      for level in levels:\n",
        "        gold = evs.StdHumanScoreName(level) if gold_name == 'std' else gold_name\n",
        "        for avg in 'none', 'sys', 'item':\n",
        "          if (level == 'sys' or seg_level_no_avg) and avg != 'none': continue\n",
        "          for human in True, False:\n",
        "            if human == True and len(evs.ref_names) == 1: continue  # Single ref\n",
        "            for corr in 'pearson', 'kendall':             \n",
        "              corr_fcn = {'pearson': scipy.stats.pearsonr,\n",
        "                          'kendall': scipy.stats.kendalltau}[corr]\n",
        "              taskname = data.MakeTaskName(\n",
        "                  'wmt22', lp, domain, level, human, avg, corr, k, gold,\n",
        "                   main_refs, close_refs, False, primary=primary_only) \n",
        "              print(taskname)\n",
        "              corrs = data.GetCorrelations(\n",
        "                  evs=evs, level=level, main_refs={evs.std_ref},\n",
        "                  close_refs=close_refs, include_human=human,\n",
        "                  include_outliers=False, gold_name=gold_name,\n",
        "                  primary_metrics=primary_only, domain=domain)\n",
        "              metrics, sig_matrix = data.CompareMetrics(\n",
        "                  corrs, corr_fcn, average_by=avg, k=k, pval=0.05)\n",
        "              # Make compatible with accuracy results.\n",
        "              metrics = {evs.DisplayName(m): v for m, v in metrics.items()}\n",
        "              results[taskname] = reformat((metrics, sig_matrix))\n",
        "\n",
        "  return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_whPvUto9yP"
      },
      "outputs": [],
      "source": [
        "# @title Load data\n",
        "\n",
        "import sys\n",
        "\n",
        "eval_sets = {}\n",
        "for lp in meta_info.DATA['wmt22']:\n",
        "  print(lp, file=sys.stderr)\n",
        "  eval_sets[lp] = data.EvalSet('wmt22', lp, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAwVTuSlHz2g"
      },
      "outputs": [],
      "source": [
        "# @title Define more global vars\n",
        "\n",
        "focus_lps = ['en-de', 'en-ru', 'zh-en']\n",
        "focus_first_lps = focus_lps + [lp for lp in eval_sets if lp not in focus_lps]\n",
        "\n",
        "# Define order of attributes for grouping, etc.\n",
        "main_attributes = ['lang', 'domain', 'level', 'human', 'avg_by', 'corr']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FU-pIhrkZft"
      },
      "source": [
        "# System scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW0H7g-a6Mj_"
      },
      "outputs": [],
      "source": [
        "# @title System performance\n",
        "\n",
        "def rank_systems(evs, system_list, scorer):\n",
        "  \"\"\"Get map from sys-\u003e(rank, score) for each sys in system list.\"\"\"\n",
        "  score_map = evs.Scores('sys', scorer)\n",
        "  pairs_to_sort = []\n",
        "  for sy in system_list:\n",
        "    sys_score = score_map[sy][0] if sy in score_map else None\n",
        "    if sys_score is None:\n",
        "      sys_score = -1000\n",
        "    pairs_to_sort.append((sys_score, sy))\n",
        "  ranked_pairs = enumerate(sorted(pairs_to_sort, reverse=True))\n",
        "  return {sy: (i + 1, score) for i, (score, sy) in ranked_pairs}\n",
        "\n",
        "def format(scorer, score):\n",
        "  if scorer == 'mqm':\n",
        "    return f'{-score:\u003c0.3f}'\n",
        "  elif scorer == 'wmt-z':\n",
        "    return f'{score:\u003c0.3f}'\n",
        "  elif scorer.startswith(('BLEURT', 'COMET')):\n",
        "    score *= 100.0\n",
        "  return f'{score:\u003c0.1f}'\n",
        "\n",
        "metrics = ['BLEU', 'chrF', 'BLEURT-20', 'COMET-20']\n",
        "\n",
        "for lp in focus_first_lps:\n",
        "  evs = eval_sets[lp]\n",
        "\n",
        "  std_gold = evs.StdHumanScoreName('sys')\n",
        "  if not std_gold:  # Some lps have no human scores.\n",
        "    continue\n",
        "\n",
        "  std_gold_scores = evs.Scores('sys', std_gold)\n",
        "  systems = [s for s in std_gold_scores if std_gold_scores[s][0] is not None]\n",
        "\n",
        "  other_gold = [h for h in evs.human_score_names if h != std_gold]\n",
        "  full_metrics = [f'{m}-{evs.std_ref}' for m in metrics]\n",
        "\n",
        "  print(lp)\n",
        "  scorer_list = '\\t'.join([std_gold] + other_gold + metrics)\n",
        "  print(f'system\\t{scorer_list}')\n",
        "  std_gold_ranking = rank_systems(evs, systems, std_gold)\n",
        "  other_rankings = {scorer: rank_systems(evs, systems, scorer) \n",
        "                    for scorer in other_gold + full_metrics}\n",
        "\n",
        "  for sys, (rank, score) in std_gold_ranking.items():\n",
        "    print(f'{sys}\\t{format(std_gold, score)} ({rank})', end='\\t')\n",
        "    for scorer in other_gold + full_metrics:\n",
        "      rank, sc = other_rankings[scorer][sys]\n",
        "      if sc == -1000:\n",
        "        print('---', end='\\t')\n",
        "      else:   \n",
        "        print(f'{format(scorer, sc)} ({rank})', end='\\t')\n",
        "    print('')\n",
        "  print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA2E1olK4ol9"
      },
      "outputs": [],
      "source": [
        "# @title Per-domain system performance\n",
        "\n",
        "def _num(v):\n",
        "  return v if v is not None else -1000\n",
        "\n",
        "def _pos(v):\n",
        "  return -v if v \u003c 0 else v\n",
        "\n",
        "for lp in focus_lps:\n",
        "  evs = eval_sets[lp]\n",
        "\n",
        "  human_scores = ['mqm']\n",
        "  if lp == 'en-ru':\n",
        "    human_scores.append('mqm.unb')  # Unbabel scores w/ Google weights.\n",
        "\n",
        "  for gold in human_scores:\n",
        "  \n",
        "    # Sort systems by global MQM score.\n",
        "    scores = evs.Scores('sys', gold)\n",
        "    scores = sorted(((s, _num(v[0])) for s, v in scores.items()),\n",
        "                    key=lambda x: -x[1])\n",
        "    scores = {k: [v] for k, v in scores}\n",
        "   \n",
        "    # Add domain-specfic MQM scores.\n",
        "    domain_header = ''\n",
        "    if 'domain' in evs.levels:\n",
        "      domain_header = ' '.join(evs.domain_names) + ' '\n",
        "      for i, d in enumerate(evs.domain_names):\n",
        "        for system in scores:\n",
        "          scores[system].append(_num(evs.Scores('domain', gold)[system][i]))\n",
        "\n",
        "    # Add system-level scores for selected metrics.\n",
        "    metrics = ['BLEU', 'chrF', 'BLEURT-20', 'COMET-20']\n",
        "    for metric in metrics:\n",
        "      metric_scores = evs.Scores('sys', f'{metric}-{evs.std_ref}')\n",
        "      assert metric_scores, metric\n",
        "      for s in scores:\n",
        "        scores[s].append(metric_scores[s][0] if s in metric_scores else 0)\n",
        "\n",
        "    print(lp, gold)\n",
        "    print(f'sys ALL {domain_header}{\" \".join(metrics)}')\n",
        "    for sysname in scores:\n",
        "      print(sysname, ' '.join(f'{_pos(s):0.3f}' for s in scores[sysname]))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHs6CEkOkKRV"
      },
      "source": [
        "# Main results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_4leYX1LAwC"
      },
      "outputs": [],
      "source": [
        "# @title MQM correlations for primary metrics\n",
        "\n",
        "# This is VERY slow unless k is \u003c 5, mostly just provided for example.\n",
        "# Main results are computed in parallel using mtme, and read in using the cell\n",
        "# below. They should exactly match the results computed here, modulo variance in\n",
        "# rank assignment due to sampling if k \u003e 0.\n",
        "\n",
        "main_results = eval_metrics(\n",
        "    eval_sets, ['en-de', 'en-ru', 'zh-en'], ['sys', 'seg'], \n",
        "    primary_only=True, k=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQnjEcoULMUx"
      },
      "outputs": [],
      "source": [
        "# @title Read pre-computed MQM correlations for primary metrics\n",
        "\n",
        "# Also see above cell.\n",
        "# Runs were generated by wmt22-metric-ranking.sh.\n",
        "\n",
        "corpus = '/usr/local/google/home/fosterg/corpora/mt-metrics-eval/work'\n",
        "run = '/metric-ranking.wmt22'\n",
        "\n",
        "main_results = {}  # task -\u003e metric -\u003e (rank, corr, sigs)\n",
        "for filename in glob.glob(f'{corpus}/{run}/task*.out'):\n",
        "  with open(filename) as f:\n",
        "    lines = [line.strip() for line in f]\n",
        "  taskname = lines[0]\n",
        "  print(taskname)\n",
        "  res = {}\n",
        "  for line in lines[1:]:\n",
        "    m, rank, corr, sigs = line.split(' ', maxsplit=3)\n",
        "    res[m] = (int(rank), float(corr), sigs)\n",
        "  main_results[taskname] = res \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5vTFe1koCpa"
      },
      "outputs": [],
      "source": [
        "# @title Print MQM sig matrices for primary metrics\n",
        "\n",
        "# Canonical task order is a sort by main attributes.\n",
        "ordered_tasks = sort_by_attrs(main_results, main_attributes)\n",
        "\n",
        "# This works with results from borg or from eval_metrics() above.\n",
        "for taskname in ordered_tasks:\n",
        "  print(taskname)\n",
        "  for m, (rank, corr, sigs) in main_results[taskname].items():\n",
        "    print(m, rank, corr, sigs)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErnCWrxpgAWW"
      },
      "outputs": [],
      "source": [
        "# @title Compute global task weights\n",
        "\n",
        "ordered_attributes = [\n",
        "    'test_set', 'lang', 'domain', 'level', 'human', 'avg_by', 'corr']\n",
        "\n",
        "def distribute_mass(task_names, attr_list, weight):\n",
        "  \"\"\"Recursively distribute mass to tasks according to ordered attributes.\"\"\"\n",
        "  if len(attr_list) == 0:\n",
        "    weight /= len(task_names)\n",
        "    return {name: weight for name in task_names}\n",
        "  partition = partition_by_attribute(task_names, attr_list[0])\n",
        "  weight /= len(partition)\n",
        "  weight_map = {}\n",
        "  for attr_val, names in partition.items():\n",
        "    sub_map = distribute_mass(names, attr_list[1:], weight)\n",
        "    weight_map.update(sub_map)\n",
        "  return weight_map  # task -\u003e weight\n",
        "\n",
        "task_weights = distribute_mass(main_results, ordered_attributes, 1.0)\n",
        "\n",
        "# Check and print.\n",
        "total_weight = 0\n",
        "for task, wt in task_weights.items():\n",
        "  print(task, wt)\n",
        "  total_weight += wt\n",
        "print(f'{total_weight=}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjfqtVqAsI_O"
      },
      "outputs": [],
      "source": [
        "# @title Average ranks for various MQM task partitions\n",
        "\n",
        "# These are the main results for the eval.\n",
        "\n",
        "def rank_metrics(tasknames, results, task_weights):\n",
        "  \"\"\"Return metric -\u003e avg_rank map, rank==None for metrics not in all tasks.\"\"\"\n",
        "  ranks, counts = {}, {}\n",
        "  total_weight = 0\n",
        "  for task in tasknames:\n",
        "    total_weight += task_weights[task]\n",
        "    for metric, (rank, _, _) in results[task].items():\n",
        "      if metric not in ranks:\n",
        "        ranks[metric], counts[metric] = 0, 0\n",
        "      ranks[metric] += task_weights[task] * rank\n",
        "      counts[metric] += 1\n",
        "\n",
        "  def _key(metric_rank):\n",
        "    metric, rank = metric_rank\n",
        "    # Metrics that don't have values for all tasks go last.\n",
        "    return rank if counts[metric] == len(tasknames) else 1000000\n",
        "  ranks = dict(sorted(ranks.items(), key=_key))\n",
        "\n",
        "  renorm = 1 / total_weight\n",
        "  for m in ranks:\n",
        "    if counts[m] != len(tasknames):\n",
        "      ranks[m] = None\n",
        "    else:\n",
        "      ranks[m] = renorm * ranks[m]\n",
        "  return ranks\n",
        "\n",
        "def display_rank(ranking, metric):\n",
        "  if metric in ranking and ranking[metric] is not None:\n",
        "    return f'{ranking[metric]:0.2f}'\n",
        "  else:\n",
        "    return 'None'\n",
        "\n",
        "# Compute ranking for partitions according to selected attributes.\n",
        "ranking_by_attr = {}  # attr/val -\u003e metric -\u003e rank\n",
        "for attr in ['test_set', 'lang', 'domain', 'level', 'human', 'corr']:\n",
        "  for val, tasks in partition_by_attribute(main_results, attr).items():\n",
        "    ranking_by_attr[f'{attr}={val}'] = rank_metrics(\n",
        "        tasks, main_results, task_weights)\n",
        "\n",
        "prime = 'test_set=wmt22'\n",
        "attr_vals = [k for k in ranking_by_attr if k != prime]\n",
        "print('metric', prime, ' '.join(attr_vals))\n",
        "for metric in ranking_by_attr[prime]:\n",
        "  ranks = [display_rank(ranking_by_attr[prime], metric)]\n",
        "  for av in attr_vals:\n",
        "    ranks.append(display_rank(ranking_by_attr[av], metric))\n",
        "  print(metric, ' '.join(ranks))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC393QtEWi0P"
      },
      "outputs": [],
      "source": [
        "# @title Print raw MQM ranking results\n",
        "\n",
        "attrs = ['lang', 'domain', 'level', 'human', 'corr']\n",
        "ordered_tasks = sort_by_attrs(main_results, attrs)\n",
        "ordered_metrics = ranking_by_attr['test_set=wmt22']\n",
        "\n",
        "# Print big table\n",
        "for attr in attrs:\n",
        "  print(attr, ' '.join([get_val(task, attr) for task in ordered_tasks]))\n",
        "for metric in ordered_metrics:\n",
        "  print(metric, end='')\n",
        "  for task in ordered_tasks:\n",
        "    if metric in main_results[task]:\n",
        "      print(f' {main_results[task][metric][0]}', end='')\n",
        "    else:\n",
        "      print(' --', end='')\n",
        "  print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlmWuSzgj3sN"
      },
      "source": [
        "# Raw correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3fzhvFWT0KS"
      },
      "outputs": [],
      "source": [
        "# @title Utils for raw correlations\n",
        "\n",
        "def get_canonical_metric_list(results_map, attr_vals):\n",
        "  \"\"\"Get canonical metric order from a given task in a results map.\"\"\"\n",
        "\n",
        "  # Find unique task designated by attr_val pairs.\n",
        "  res = results_map\n",
        "  for attr, val in attr_vals:\n",
        "    res = partition_by_attribute(res, attr)[val]\n",
        "  assert len(res) == 1\n",
        "  main_task = res[0]\n",
        "  # print(task)\n",
        "  metric_list = list(results_map[main_task])\n",
        "\n",
        "  # Add in metrics that aren't available for selected task\n",
        "  for task in results_map:\n",
        "    for metric in results_map[task]:\n",
        "      if metric not in metric_list:\n",
        "        metric_list.append(metric)\n",
        "\n",
        "  return metric_list, main_task\n",
        "\n",
        "def print_results_table(results_map, sortby_attrs, metric_list):\n",
        "  \"\"\"Print all results in given map, using specified tasks and metric order.\"\"\"\n",
        "\n",
        "  # Sort tasks by given attributes\n",
        "  ordered_tasks = sort_by_attrs(results_map, sortby_attrs)\n",
        "\n",
        "  # Print header lines\n",
        "  for attr in sortby_attrs:\n",
        "    print(attr, ' '.join([get_val(task, attr) for task in ordered_tasks]))\n",
        "\n",
        "  # Table content.\n",
        "  for metric in metric_list:\n",
        "    print(metric, end='')\n",
        "    for task in ordered_tasks:\n",
        "      if metric in results_map[task]:\n",
        "        print(f' {results_map[task][metric][1]}', end='')\n",
        "      else:\n",
        "        print(' --', end='')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPa8gKZiILyB"
      },
      "outputs": [],
      "source": [
        "# @title Correlations using MQM\n",
        "\n",
        "# Differences from main results:\n",
        "# - no significance\n",
        "# - correlations for all metrics, not just primary submissions\n",
        "\n",
        "mqm_results = eval_metrics(\n",
        "    eval_sets, focus_lps, ['sys', 'seg'], primary_only=False, k=0,\n",
        "     gold_name='mqm')\n",
        "print()\n",
        "\n",
        "ordered_metrics, _ = get_canonical_metric_list(mqm_results, [('corr', 'accuracy')])\n",
        "print_results_table(mqm_results, main_attributes, ordered_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_8fryYu7nrT"
      },
      "outputs": [],
      "source": [
        "# @title Correlations using Appraise\n",
        "\n",
        "appraise_lps = ['cs-uk', 'en-cs', 'en-de', 'en-hr', 'en-ja', 'en-liv', 'en-ru',\n",
        "                'en-uk', 'en-zh', 'liv-en', 'sah-ru', 'uk-cs', 'zh-en']\n",
        "\n",
        "appraise_results = eval_metrics(\n",
        "    eval_sets, appraise_lps, ['sys', 'seg'], primary_only=False, k=0,\n",
        "    gold_name='wmt-appraise', include_domains=False, seg_level_no_avg=True,\n",
        "    include_human_with_acc=True)\n",
        "print()\n",
        "\n",
        "ordered_metrics, _ = get_canonical_metric_list(\n",
        "    appraise_results, [('corr', 'accuracy'), ('human', 'True')])\n",
        "attrs = ['lang', 'level', 'human', 'corr'] \n",
        "print_results_table(appraise_results, attrs, ordered_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DTEeOI42uSs"
      },
      "outputs": [],
      "source": [
        "# @title Correlations using DA\n",
        "\n",
        "da_lps = ['cs-en', 'de-en', 'ja-en', 'ru-en', 'uk-en', 'zh-en']\n",
        "\n",
        "da_results = eval_metrics(\n",
        "    eval_sets, da_lps, ['sys', 'seg'], primary_only=False, k=0,\n",
        "    gold_name='wmt', include_domains=False, seg_level_no_avg=True)\n",
        "print()\n",
        "\n",
        "ordered_metrics, _ = get_canonical_metric_list(da_results, [('corr', 'accuracy')])\n",
        "attrs = ['lang', 'level', 'human', 'corr'] \n",
        "print_results_table(da_results, attrs, ordered_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn4ijtNB8-uI"
      },
      "outputs": [],
      "source": [
        "# @title Latex tables for appendices\n",
        "\n",
        "# Depends on the cells above to compute: \n",
        "# mqm_results, da_results, appraise_results.\n",
        "\n",
        "BASELINES = {'f101spBLEU', 'f200spBLEU', 'chrF', 'BERTScore', 'YiSi-1', 'BLEU',\n",
        "             'BLEURT-20', 'COMET-20', 'COMET-QE'}\n",
        "\n",
        "def make_table(res, level='sys', corr='pearson', acc_human='True',\n",
        "               other_humans=None):\n",
        "  \"\"\"Make Latex table from results map: task -\u003e metric -\u003e score.\"\"\"\n",
        "\n",
        "  metrics_in_order, main_task = get_canonical_metric_list(\n",
        "      res, [('corr', 'accuracy'), ('human', acc_human)])\n",
        "  \n",
        "  # Pick out subset of tasks in order.\n",
        "  if other_humans is None:\n",
        "    other_humans = ['True', 'False']\n",
        "  tasks_to_print = {}  # task -\u003e metric -\u003e score\n",
        "  for task in res:\n",
        "    if task == main_task:\n",
        "      tasks_to_print[task] = res[task]\n",
        "    else:\n",
        "      attrs = task_to_dict(task)\n",
        "      if (attrs['level'] == level and attrs['domain'] == 'None' and \n",
        "          attrs['corr'] == corr and attrs['avg_by'] == 'none' and\n",
        "          attrs['human'] in other_humans):\n",
        "        tasks_to_print[task] = res[task]\n",
        "\n",
        "  # Print header\n",
        "  print(f'level={level} corr={corr}')\n",
        "  for task in tasks_to_print:\n",
        "    lang = get_val(task, 'lang')\n",
        "    if lang.startswith('['):\n",
        "      print('acc', end='')\n",
        "    else:\n",
        "      print(f' \u0026 {lang}', end='')\n",
        "  print('\\\\\\\\')\n",
        "  print(' \u0026 '.join(get_val(t, 'human') for t in tasks_to_print), '\\\\\\\\')\n",
        "\n",
        "  # Contents\n",
        "  for m in metrics_in_order:\n",
        "    orig_m = m\n",
        "    if m.endswith('[noref]'):\n",
        "      m = m[:-len('[noref]')]\n",
        "      star = '*'\n",
        "    else:\n",
        "      star = ''\n",
        "    if m.startswith('*'):\n",
        "      m = m[1:] + star\n",
        "    else:\n",
        "      if m in BASELINES:\n",
        "        m = f'\\\\underline{{{m}{star}}}'\n",
        "      else:\n",
        "        m = f'\\\\textbf{{{m}{star}}}'\n",
        "    print(f'{m}', end='')\n",
        "\n",
        "    for task in tasks_to_print:\n",
        "      val = f'{tasks_to_print[task][orig_m][1]:0.3f}' if orig_m in tasks_to_print[task] else '--'\n",
        "      print(f' \u0026 {val}', end='')\n",
        "    print('\\\\\\\\')\n",
        "  print()\n",
        "\n",
        "\n",
        "print('MQM results')\n",
        "make_table(mqm_results, 'sys', 'pearson', 'False')\n",
        "print('MQM results')\n",
        "make_table(mqm_results, 'seg', 'kendall', 'False')\n",
        "\n",
        "print('DA results')\n",
        "make_table(da_results, 'sys', 'pearson', 'False', ['False'])\n",
        "print('DA results')\n",
        "make_table(da_results, 'seg', 'kendall', 'False', ['False'])\n",
        "\n",
        "print('Appraise results')\n",
        "make_table(appraise_results, 'sys', 'pearson', 'True', ['False'])\n",
        "print('Appraise results')\n",
        "make_table(appraise_results, 'seg', 'kendall', 'True', ['False'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZEX7OImjm1N"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijEMODMxfCuZ"
      },
      "outputs": [],
      "source": [
        "# @title Unbabel vs Google MQM stats for en-ru\n",
        "\n",
        "# This computes system-level Pearson and Kendall for all metrics\n",
        "# with both MQM scoring schemes.\n",
        "\n",
        "goog_results = eval_metrics(\n",
        "    eval_sets, ['en-ru'], ['sys'], primary_only=True, k=100, gold_name='mqm', \n",
        "    include_domains=False)\n",
        "unb_results = eval_metrics(\n",
        "    eval_sets, ['en-ru'], ['sys'], primary_only=True, k=100, gold_name='mqm.unb',\n",
        "    include_domains=False)\n",
        "\n",
        "unb_pearson = partition_by_attribute(unb_results, 'corr')['pearson'][0]\n",
        "unb_kendall = partition_by_attribute(unb_results, 'corr')['kendall'][0]\n",
        "goog_pearson = partition_by_attribute(goog_results, 'corr')['pearson'][0]\n",
        "goog_kendall = partition_by_attribute(goog_results, 'corr')['kendall'][0]\n",
        "\n",
        "print('metric', \n",
        "      'unb-pears-rank unb-pears-corr unb-kend-rank unb-kend-corr ' \n",
        "      'goog-pears-rank goog-pears-corr goog-kend-rank goog-kend-corr')\n",
        "for m in unb_results[unb_pearson]:\n",
        "  upr, upc, _ = unb_results[unb_pearson][m]\n",
        "  ukr, ukc, _ = unb_results[unb_kendall][m]\n",
        "  gpr, gpc, _ = goog_results[goog_pearson][m]\n",
        "  gkr, gkc, _ = goog_results[goog_kendall][m]\n",
        "  print(m, \n",
        "        f'{upr} {upc:0.3f} {ukr} {ukc:0.3f} {gpr} {gpc:0.3f} {gkr} {gkc:0.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yGDdCpRykax"
      },
      "outputs": [],
      "source": [
        "# @title Accuracy vs Kendall\n",
        "\n",
        "# en-de only, testing the difference between these two tasks.\n",
        "# (The ranking is identical and significance clusters are almost identical.)\n",
        "\n",
        "kf = stats.KendallLike(thresh=0)\n",
        "def acc(v1, v2):\n",
        "  _, num_pairs, concordant, _ = kf.Corr(v1, v2)\n",
        "  return concordant / num_pairs, num_pairs\n",
        "\n",
        "evs = eval_sets['en-de']\n",
        "\n",
        "corrs = data.GetCorrelations(\n",
        "    evs, level='sys', main_refs = {evs.std_ref}, close_refs=set(),\n",
        "    include_human=True, include_outliers=False, gold_name='mqm',\n",
        "    primary_metrics=True)\n",
        "\n",
        "# Kendall results\n",
        "res, sig_matrix = data.CompareMetrics(corrs, scipy.stats.kendalltau, k=1000)\n",
        "for m, (corr, rank) in res.items():\n",
        "  print(m, rank, f'{corr:0.3f}')\n",
        "print()\n",
        "\n",
        "# Accuracy results\n",
        "res, sig_matrix = data.CompareMetrics(corrs, acc, k=1000)\n",
        "for m, (corr, rank) in res.items():\n",
        "  print(m, rank, f'{corr:0.3f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1Y2uR4EnLUgx_5xfo4kO6x-GvAD4uDCeP",
          "timestamp": 1668813444293
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
